<?xml version="1.0" encoding="UTF-8"?>

<text>
 <s id="1">Every day, every week, we agree to terms and conditions.</s>
 <s id="2">And when we do this, we provide companies with the lawful right to do whatever they want with our data and with the data of our children.</s>
 <s id="3">Which makes us wonder: how much data are we giving away of children, and what are its implications?</s>
 <s id="4">I'm an anthropologist, and I'm also the mother of two little girls.</s>
 <s id="5">And I started to become interested in this question in 2015 when I suddenly realized that there were vast -- almost unimaginable amounts of data traces that are being produced and collected about children.</s>
 <s id="6">So I launched a research project, which is called Child Data Citizen, and I aimed at filling in the blank.</s>
 <s id="7">Now you may think that I'm here to blame you for posting photos of your children on social media, but that's not really the point.</s>
 <s id="8">The problem is way bigger than so-called "sharenting."</s>
 <s id="9">This is about systems, not individuals.</s>
 <s id="10">You and your habits are not to blame.</s>
 <s id="11">For the very first time in history, we are tracking the individual data of children from long before they're born -- sometimes from the moment of conception, and then throughout their lives.</s>
 <s id="12">You see, when parents decide to conceive, they go online to look for "ways to get pregnant," or they download ovulation-tracking apps.</s>
 <s id="13">When they do get pregnant, they post ultrasounds of their babies on social media, they download pregnancy apps or they consult Dr. Google for all sorts of things, like, you know -- for "miscarriage risk when flying" or "abdominal cramps in early pregnancy."</s>
 <s id="14">I know because I've done it -- and many times.</s>
 <s id="15">And then, when the baby is born, they track every nap, every feed, every life event on different technologies.</s>
 <s id="16">And all of these technologies transform the baby's most intimate behavioral and health data into profit by sharing it with others.</s>
 <s id="17">So to give you an idea of how this works, in 2019, the British Medical Journal published research that showed that out of 24 mobile health apps, 19 shared information with third parties.</s>
 <s id="18">And these third parties shared information with 216 other organizations.</s>
 <s id="19">Of these 216 other fourth parties, only three belonged to the health sector.</s>
 <s id="20">The other companies that had access to that data were big tech companies like Google, Facebook or Oracle, they were digital advertising companies and there was also a consumer credit reporting agency.</s>
 <s id="21">So you get it right: ad companies and credit agencies may already have data points on little babies.</s>
 <s id="22">But mobile apps, web searches and social media are really just the tip of the iceberg, because children are being tracked by multiple technologies in their everyday lives.</s>
 <s id="23">They're tracked by home technologies and virtual assistants in their homes.</s>
 <s id="24">They're tracked by educational platforms and educational technologies in their schools.</s>
 <s id="25">They're tracked by online records and online portals at their doctor's office.</s>
 <s id="26">They're tracked by their internet-connected toys, their online games and many, many, many, many other technologies.</s>
 <s id="27">So during my research, a lot of parents came up to me and they were like, "So what?</s>
 <s id="28">Why does it matter if my children are being tracked?</s>
 <s id="29">We've got nothing to hide."</s>
 <s id="30">Well, it matters.</s>
 <s id="31">It matters because today individuals are not only being tracked, they're also being profiled on the basis of their data traces.</s>
 <s id="32">Artificial intelligence and predictive analytics are being used to harness as much data as possible of an individual life from different sources: family history, purchasing habits, social media comments.</s>
 <s id="33">And then they bring this data together to make data-driven decisions about the individual.</s>
 <s id="34">And these technologies are used everywhere.</s>
 <s id="35">Banks use them to decide loans.</s>
 <s id="36">Insurance uses them to decide premiums.</s>
 <s id="37">Recruiters and employers use them to decide whether one is a good fit for a job or not.</s>
 <s id="38">Also the police and courts use them to determine whether one is a potential criminal or is likely to recommit a crime.</s>
 <s id="39">We have no knowledge or control over the ways in which those who buy, sell and process our data are profiling us and our children.</s>
 <s id="40">But these profiles can come to impact our rights in significant ways.</s>
 <s id="41">To give you an example, in 2018 the "New York Times" published the news that the data that had been gathered through online college-planning services -- that are actually completed by millions of high school kids across the US who are looking for a college program or a scholarship -- had been sold to educational data brokers.</s>
 <s id="42">Now, researchers at Fordham who studied educational data brokers revealed that these companies profiled kids as young as two on the basis of different categories: ethnicity, religion, affluence, social awkwardness and many other random categories.</s>
 <s id="43">And then they sell these profiles together with the name of the kid, their home address and the contact details to different companies, including trade and career institutions, student loans and student credit card companies.</s>
 <s id="44">To push the boundaries, the researchers at Fordham asked an educational data broker to provide them with a list of 14-to-15-year-old girls who were interested in family planning services.</s>
 <s id="45">The data broker agreed to provide them the list.</s>
 <s id="46">So imagine how intimate and how intrusive that is for our kids.</s>
 <s id="47">But educational data brokers are really just an example.</s>
 <s id="48">The truth is that our children are being profiled in ways that we cannot control but that can significantly impact their chances in life.</s>
 <s id="49">So we need to ask ourselves: can we trust these technologies when it comes to profiling our children?</s>
 <s id="50">Can we?</s>
 <s id="51">My answer is no.</s>
 <s id="52">As an anthropologist, I believe that artificial intelligence and predictive analytics can be great to predict the course of a disease or to fight climate change.</s>
 <s id="53">But we need to abandon the belief that these technologies can objectively profile humans and that we can rely on them to make data-driven decisions about individual lives.</s>
 <s id="54">Because they can't profile humans.</s>
 <s id="55">Data traces are not the mirror of who we are.</s>
 <s id="56">Humans think one thing and say the opposite, feel one way and act differently.</s>
 <s id="57">Algorithmic predictions or our digital practices cannot account for the unpredictability and complexity of human experience.</s>
 <s id="58">But on top of that, these technologies are always -- always -- in one way or another, biased.</s>
 <s id="59">You see, algorithms are by definition sets of rules or steps that have been designed to achieve a specific result, OK?</s>
 <s id="60">But these sets of rules or steps cannot be objective, because they've been designed by human beings within a specific cultural context and are shaped by specific cultural values.</s>
 <s id="61">So when machines learn, they learn from biased algorithms, and they often learn from biased databases as well.</s>
 <s id="62">At the moment, we're seeing the first examples of algorithmic bias.</s>
 <s id="63">And some of these examples are frankly terrifying.</s>
 <s id="64">This year, the AI Now Institute in New York published a report that revealed that the AI technologies that are being used for predictive policing have been trained on "dirty" data.</s>
 <s id="65">This is basically data that had been gathered during historical periods of known racial bias and nontransparent police practices.</s>
 <s id="66">Because these technologies are being trained with dirty data, they're not objective, and their outcomes are only amplifying and perpetrating police bias and error.</s>
 <s id="67">So I think we are faced with a fundamental problem in our society.</s>
 <s id="68">We are starting to trust technologies when it comes to profiling human beings.</s>
 <s id="69">We know that in profiling humans, these technologies are always going to be biased and are never really going to be accurate.</s>
 <s id="70">So what we need now is actually political solution.</s>
 <s id="71">We need governments to recognize that our data rights are our human rights.</s>
 <s id="72">(Applause and cheers) Until this happens, we cannot hope for a more just future.</s>
 <s id="73">I worry that my daughters are going to be exposed to all sorts of algorithmic discrimination and error.</s>
 <s id="74">You see the difference between me and my daughters is that there's no public record out there of my childhood.</s>
 <s id="75">There's certainly no database of all the stupid things that I've done and thought when I was a teenager.</s>
 <s id="76">(Laughter) But for my daughters this may be different.</s>
 <s id="77">The data that is being collected from them today may be used to judge them in the future and can come to prevent their hopes and dreams.</s>
 <s id="78">I think that's it's time.</s>
 <s id="79">It's time that we all step up.</s>
 <s id="80">It's time that we start working together as individuals, as organizations and as institutions, and that we demand greater data justice for us and for our children before it's too late.</s>
 <s id="81">Thank you.</s>
 <s id="82">(Applause)</s>
</text>
