<?xml version="1.0" encoding="UTF-8"?>

<text>
 <s id="1">Power.</s>
 <s id="2">That is the word that comes to mind.</s>
 <s id="3">We're the new technologists.</s>
 <s id="4">We have a lot of data, so we have a lot of power.</s>
 <s id="5">How much power do we have?</s>
 <s id="6">Scene from a movie: "Apocalypse Now" -- great movie.</s>
 <s id="7">We've got to get our hero, Captain Willard, to the mouth of the Nung River so he can go pursue Colonel Kurtz.</s>
 <s id="8">The way we're going to do this is fly him in and drop him off.</s>
 <s id="9">So the scene: the sky is filled with this fleet of helicopters carrying him in.</s>
 <s id="10">And there's this loud, thrilling music in the background, this wild music.</s>
 <s id="11">♫ Dum da ta da dum ♫ ♫ Dum da ta da dum ♫ ♫ Da ta da da ♫ That's a lot of power.</s>
 <s id="12">That's the kind of power I feel in this room.</s>
 <s id="13">That's the kind of power we have because of all of the data that we have.</s>
 <s id="14">Let's take an example.</s>
 <s id="15">What can we do with just one person's data?</s>
 <s id="16">What can we do with that guy's data?</s>
 <s id="17">I can look at your financial records.</s>
 <s id="18">I can tell if you pay your bills on time.</s>
 <s id="19">I know if you're good to give a loan to.</s>
 <s id="20">I can look at your medical records; I can see if your pump is still pumping -- see if you're good to offer insurance to.</s>
 <s id="21">I can look at your clicking patterns.</s>
 <s id="22">When you come to my website, I actually know what you're going to do already because I've seen you visit millions of websites before.</s>
 <s id="23">And I'm sorry to tell you, you're like a poker player, you have a tell.</s>
 <s id="24">I can tell with data analysis what you're going to do before you even do it.</s>
 <p id="p25">
  <s id="25.1">I know what you like.</s>
  <s id="25.2">I know who you are, and that's even before I look at your mail or your phone.</s>
 </p>
 <s id="26">Those are the kinds of things we can do with the data that we have.</s>
 <s id="27">But I'm not actually here to talk about what we can do.</s>
 <s id="28">I'm here to talk about what we should do.</s>
 <s id="29">What's the right thing to do?</s>
 <s id="30">Now I see some puzzled looks like, "Why are you asking us what's the right thing to do?</s>
 <p id="p31">
  <s id="31.1">We're just building this stuff.</s>
  <s id="31.2">Somebody else is using it."</s>
 </p>
 <s id="32">Fair enough.</s>
 <s id="33">But it brings me back.</s>
 <s id="34">I think about World War II -- some of our great technologists then, some of our great physicists, studying nuclear fission and fusion -- just nuclear stuff.</s>
 <s id="35">We gather together these physicists in Los Alamos to see what they'll build.</s>
 <s id="36">We want the people building the technology thinking about what we should be doing with the technology.</s>
 <s id="37">So what should we be doing with that guy's data?</s>
 <s id="38">Should we be collecting it, gathering it, so we can make his online experience better?</s>
 <s id="39">So we can make money?</s>
 <s id="40">So we can protect ourselves if he was up to no good?</s>
 <s id="41">Or should we respect his privacy, protect his dignity and leave him alone?</s>
 <s id="42">Which one is it?</s>
 <s id="43">How should we figure it out?</s>
 <p id="p44">
  <s id="44.1">I know: crowdsource.</s>
  <s id="44.2">Let's crowdsource this.</s>
 </p>
 <s id="45">So to get people warmed up, let's start with an easy question -- something I'm sure everybody here has an opinion about: iPhone versus Android.</s>
 <s id="46">Let's do a show of hands -- iPhone.</s>
 <s id="47">Uh huh.</s>
 <s id="48">Android.</s>
 <s id="49">You'd think with a bunch of smart people we wouldn't be such suckers just for the pretty phones.</s>
 <s id="50">(Laughter) Next question, a little bit harder.</s>
 <s id="51">Should we be collecting all of that guy's data to make his experiences better and to protect ourselves in case he's up to no good?</s>
 <s id="52">Or should we leave him alone?</s>
 <s id="53">Collect his data.</s>
 <s id="54">Leave him alone.</s>
 <p id="p55">
  <s id="55.1">You're safe.</s>
  <s id="55.2">It's fine.</s>
 </p>
 <s id="56">(Laughter) Okay, last question -- harder question -- when trying to evaluate what we should do in this case, should we use a Kantian deontological moral framework, or should we use a Millian consequentialist one?</s>
 <s id="57">Kant.</s>
 <s id="58">Mill.</s>
 <s id="59">Not as many votes.</s>
 <s id="60">(Laughter) Yeah, that's a terrifying result.</s>
 <s id="61">Terrifying, because we have stronger opinions about our hand-held devices than about the moral framework we should use to guide our decisions.</s>
 <s id="62">How do we know what to do with all the power we have if we don't have a moral framework?</s>
 <s id="63">We know more about mobile operating systems, but what we really need is a moral operating system.</s>
 <s id="64">What's a moral operating system?</s>
 <s id="65">We all know right and wrong, right?</s>
 <s id="66">You feel good when you do something right, you feel bad when you do something wrong.</s>
 <s id="67">Our parents teach us that: praise with the good, scold with the bad.</s>
 <s id="68">But how do we figure out what's right and wrong?</s>
 <s id="69">And from day to day, we have the techniques that we use.</s>
 <s id="70">Maybe we just follow our gut.</s>
 <s id="71">Maybe we take a vote -- we crowdsource.</s>
 <s id="72">Or maybe we punt -- ask the legal department, see what they say.</s>
 <s id="73">In other words, it's kind of random, kind of ad hoc, how we figure out what we should do.</s>
 <s id="74">And maybe, if we want to be on surer footing, what we really want is a moral framework that will help guide us there, that will tell us what kinds of things are right and wrong in the first place, and how would we know in a given situation what to do.</s>
 <s id="75">So let's get a moral framework.</s>
 <s id="76">We're numbers people, living by numbers.</s>
 <s id="77">How can we use numbers as the basis for a moral framework?</s>
 <s id="78">I know a guy who did exactly that.</s>
 <s id="79">A brilliant guy -- he's been dead 2,500 years.</s>
 <s id="80">Plato, that's right.</s>
 <s id="81">Remember him -- old philosopher?</s>
 <s id="82">You were sleeping during that class.</s>
 <s id="83">And Plato, he had a lot of the same concerns that we did.</s>
 <s id="84">He was worried about right and wrong.</s>
 <s id="85">He wanted to know what is just.</s>
 <s id="86">But he was worried that all we seem to be doing is trading opinions about this.</s>
 <p id="p87">
  <s id="87.1">He says something's just.</s>
  <s id="87.2">She says something else is just.</s>
 </p>
 <s id="88">It's kind of convincing when he talks and when she talks too.</s>
 <s id="89">I'm just going back and forth; I'm not getting anywhere.</s>
 <s id="90">I don't want opinions; I want knowledge.</s>
 <s id="91">I want to know the truth about justice -- like we have truths in math.</s>
 <s id="92">In math, we know the objective facts.</s>
 <s id="93">Take a number, any number -- two.</s>
 <p id="p94">
  <s id="94.1">Favorite number.</s>
  <s id="94.2">I love that number.</s>
 </p>
 <s id="95">There are truths about two.</s>
 <s id="96">If you've got two of something, you add two more, you get four.</s>
 <s id="97">That's true no matter what thing you're talking about.</s>
 <s id="98">It's an objective truth about the form of two, the abstract form.</s>
 <s id="99">When you have two of anything -- two eyes, two ears, two noses, just two protrusions -- those all partake of the form of two.</s>
 <s id="100">They all participate in the truths that two has.</s>
 <s id="101">They all have two-ness in them.</s>
 <s id="102">And therefore, it's not a matter of opinion.</s>
 <s id="103">What if, Plato thought, ethics was like math?</s>
 <s id="104">What if there were a pure form of justice?</s>
 <s id="105">What if there are truths about justice, and you could just look around in this world and see which things participated, partook of that form of justice?</s>
 <s id="106">Then you would know what was really just and what wasn't.</s>
 <s id="107">It wouldn't be a matter of just opinion or just appearances.</s>
 <s id="108">That's a stunning vision.</s>
 <p id="p109">
  <s id="109.1">I mean, think about that.</s>
  <s id="109.2">How grand.</s>
  <s id="109.3">How ambitious.</s>
 </p>
 <s id="110">That's as ambitious as we are.</s>
 <s id="111">He wants to solve ethics.</s>
 <s id="112">He wants objective truths.</s>
 <s id="113">If you think that way, you have a Platonist moral framework.</s>
 <s id="114">If you don't think that way, well, you have a lot of company in the history of Western philosophy, because the tidy idea, you know, people criticized it.</s>
 <s id="115">Aristotle, in particular, he was not amused.</s>
 <s id="116">He thought it was impractical.</s>
 <s id="117">Aristotle said, "We should seek only so much precision in each subject as that subject allows."</s>
 <s id="118">Aristotle thought ethics wasn't a lot like math.</s>
 <s id="119">He thought ethics was a matter of making decisions in the here-and-now using our best judgment to find the right path.</s>
 <s id="120">If you think that, Plato's not your guy.</s>
 <s id="121">But don't give up.</s>
 <s id="122">Maybe there's another way that we can use numbers as the basis of our moral framework.</s>
 <s id="123">How about this: What if in any situation you could just calculate, look at the choices, measure out which one's better and know what to do?</s>
 <s id="124">That sound familiar?</s>
 <s id="125">That's a utilitarian moral framework.</s>
 <s id="126">John Stuart Mill was a great advocate of this -- nice guy besides -- and only been dead 200 years.</s>
 <s id="127">So basis of utilitarianism -- I'm sure you're familiar at least.</s>
 <s id="128">The three people who voted for Mill before are familiar with this.</s>
 <s id="129">But here's the way it works.</s>
 <s id="130">What if morals, what if what makes something moral is just a matter of if it maximizes pleasure and minimizes pain?</s>
 <s id="131">It does something intrinsic to the act.</s>
 <s id="132">It's not like its relation to some abstract form.</s>
 <s id="133">It's just a matter of the consequences.</s>
 <s id="134">You just look at the consequences and see if, overall, it's for the good or for the worse.</s>
 <p id="p135">
  <s id="135.1">That would be simple.</s>
  <s id="135.2">Then we know what to do.</s>
 </p>
 <s id="136">Let's take an example.</s>
 <s id="137">Suppose I go up and I say, "I'm going to take your phone."</s>
 <s id="138">Not just because it rang earlier, but I'm going to take it because I made a little calculation.</s>
 <s id="139">I thought, that guy looks suspicious.</s>
 <s id="140">And what if he's been sending little messages to Bin Laden's hideout -- or whoever took over after Bin Laden -- and he's actually like a terrorist, a sleeper cell.</s>
 <s id="141">I'm going to find that out, and when I find that out, I'm going to prevent a huge amount of damage that he could cause.</s>
 <s id="142">That has a very high utility to prevent that damage.</s>
 <s id="143">And compared to the little pain that it's going to cause -- because it's going to be embarrassing when I'm looking on his phone and seeing that he has a Farmville problem and that whole bit -- that's overwhelmed by the value of looking at the phone.</s>
 <s id="144">If you feel that way, that's a utilitarian choice.</s>
 <s id="145">But maybe you don't feel that way either.</s>
 <s id="146">Maybe you think, it's his phone.</s>
 <s id="147">It's wrong to take his phone because he's a person and he has rights and he has dignity, and we can't just interfere with that.</s>
 <s id="148">He has autonomy.</s>
 <s id="149">It doesn't matter what the calculations are.</s>
 <s id="150">There are things that are intrinsically wrong -- like lying is wrong, like torturing innocent children is wrong.</s>
 <s id="151">Kant was very good on this point, and he said it a little better than I'll say it.</s>
 <s id="152">He said we should use our reason to figure out the rules by which we should guide our conduct, and then it is our duty to follow those rules.</s>
 <s id="153">It's not a matter of calculation.</s>
 <s id="154">So let's stop.</s>
 <s id="155">We're right in the thick of it, this philosophical thicket.</s>
 <s id="156">And this goes on for thousands of years, because these are hard questions, and I've only got 15 minutes.</s>
 <s id="157">So let's cut to the chase.</s>
 <s id="158">How should we be making our decisions?</s>
 <s id="159">Is it Plato, is it Aristotle, is it Kant, is it Mill?</s>
 <p id="p160">
  <s id="160.1">What should we be doing?</s>
  <s id="160.2">What's the answer?</s>
 </p>
 <s id="161">What's the formula that we can use in any situation to determine what we should do, whether we should use that guy's data or not?</s>
 <s id="162">What's the formula?</s>
 <s id="163">There's not a formula.</s>
 <s id="164">There's not a simple answer.</s>
 <s id="165">Ethics is hard.</s>
 <s id="166">Ethics requires thinking.</s>
 <s id="167">And that's uncomfortable.</s>
 <s id="168">I know; I spent a lot of my career in artificial intelligence, trying to build machines that could do some of this thinking for us, that could give us answers.</s>
 <s id="169">But they can't.</s>
 <s id="170">You can't just take human thinking and put it into a machine.</s>
 <s id="171">We're the ones who have to do it.</s>
 <s id="172">Happily, we're not machines, and we can do it.</s>
 <s id="173">Not only can we think, we must.</s>
 <s id="174">Hannah Arendt said, "The sad truth is that most evil done in this world is not done by people who choose to be evil.</s>
 <s id="175">It arises from not thinking."</s>
 <s id="176">That's what she called the "banality of evil."</s>
 <s id="177">And the response to that is that we demand the exercise of thinking from every sane person.</s>
 <p id="p178">
  <s id="178.1">So let's do that.</s>
  <s id="178.2">Let's think.</s>
 </p>
 <s id="179">In fact, let's start right now.</s>
 <s id="180">Every person in this room do this: think of the last time you had a decision to make where you were worried to do the right thing, where you wondered, "What should I be doing?"</s>
 <s id="181">Bring that to mind, and now reflect on that and say, "How did I come up that decision?</s>
 <p id="p182">
  <s id="182.1">What did I do?</s>
  <s id="182.2">Did I follow my gut?</s>
 </p>
 <p id="p183">
  <s id="183.1">Did I have somebody vote on it?</s>
  <s id="183.2">Or did I punt to legal?"</s>
 </p>
 <s id="184">Or now we have a few more choices.</s>
 <s id="185">"Did I evaluate what would be the highest pleasure like Mill would?</s>
 <s id="186">Or like Kant, did I use reason to figure out what was intrinsically right?"</s>
 <p id="p187">
  <s id="187.1">Think about it.</s>
  <s id="187.2">Really bring it to mind.</s>
  <s id="187.3">This is important.</s>
 </p>
 <s id="188">It is so important we are going to spend 30 seconds of valuable TEDTalk time doing nothing but thinking about this.</s>
 <p id="p189">
  <s id="189.1">Are you ready?</s>
  <s id="189.2">Go.</s>
 </p>
 <p id="p190">
  <s id="190.1">Stop.</s>
  <s id="190.2">Good work.</s>
 </p>
 <s id="191">What you just did, that's the first step towards taking responsibility for what we should do with all of our power.</s>
 <s id="192">Now the next step -- try this.</s>
 <s id="193">Go find a friend and explain to them how you made that decision.</s>
 <p id="p194">
  <s id="194.1">Not right now.</s>
  <s id="194.2">Wait till I finish talking.</s>
 </p>
 <s id="195">Do it over lunch.</s>
 <s id="196">And don't just find another technologist friend; find somebody different than you.</s>
 <s id="197">Find an artist or a writer -- or, heaven forbid, find a philosopher and talk to them.</s>
 <s id="198">In fact, find somebody from the humanities.</s>
 <p id="p199">
  <s id="199.1">Why?</s>
  <s id="199.2">Because they think about problems differently than we do as technologists.</s>
 </p>
 <s id="200">Just a few days ago, right across the street from here, there was hundreds of people gathered together.</s>
 <s id="201">It was technologists and humanists at that big BiblioTech Conference.</s>
 <s id="202">And they gathered together because the technologists wanted to learn what it would be like to think from a humanities perspective.</s>
 <s id="203">You have someone from Google talking to someone who does comparative literature.</s>
 <s id="204">You're thinking about the relevance of 17th century French theater -- how does that bear upon venture capital?</s>
 <p id="p205">
  <s id="205.1">Well that's interesting.</s>
  <s id="205.2">That's a different way of thinking.</s>
 </p>
 <s id="206">And when you think in that way, you become more sensitive to the human considerations, which are crucial to making ethical decisions.</s>
 <s id="207">So imagine that right now you went and you found your musician friend.</s>
 <s id="208">And you're telling him what we're talking about, about our whole data revolution and all this -- maybe even hum a few bars of our theme music.</s>
 <s id="209">♫ Dum ta da da dum dum ta da da dum ♫ Well, your musician friend will stop you and say, "You know, the theme music for your data revolution, that's an opera, that's Wagner.</s>
 <s id="210">It's based on Norse legend.</s>
 <s id="211">It's Gods and mythical creatures fighting over magical jewelry."</s>
 <s id="212">That's interesting.</s>
 <s id="213">Now it's also a beautiful opera, and we're moved by that opera.</s>
 <s id="214">We're moved because it's about the battle between good and evil, about right and wrong.</s>
 <s id="215">And we care about right and wrong.</s>
 <s id="216">We care what happens in that opera.</s>
 <s id="217">We care what happens in "Apocalypse Now."</s>
 <s id="218">And we certainly care what happens with our technologies.</s>
 <s id="219">We have so much power today, it is up to us to figure out what to do, and that's the good news.</s>
 <s id="220">We're the ones writing this opera.</s>
 <s id="221">This is our movie.</s>
 <s id="222">We figure out what will happen with this technology.</s>
 <s id="223">We determine how this will all end.</s>
 <s id="224">Thank you.</s>
 <s id="225">(Applause)</s>
</text>
