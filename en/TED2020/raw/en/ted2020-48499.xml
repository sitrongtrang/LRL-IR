<?xml version="1.0" encoding="UTF-8"?>

<text>
 <s id="1">[This talk contains mature content] Five years ago, I received a phone call that would change my life.</s>
 <s id="2">I remember so vividly that day.</s>
 <s id="3">It was about this time of year, and I was sitting in my office.</s>
 <s id="4">I remember the sun streaming through the window.</s>
 <s id="5">And my phone rang.</s>
 <s id="6">And I picked it up, and it was two federal agents, asking for my help in identifying a little girl featured in hundreds of child sexual abuse images they had found online.</s>
 <s id="7">They had just started working the case, but what they knew was that her abuse had been broadcast to the world for years on dark web sites dedicated to the sexual abuse of children.</s>
 <s id="8">And her abuser was incredibly technologically sophisticated: new images and new videos every few weeks, but very few clues as to who she was or where she was.</s>
 <s id="9">And so they called us, because they had heard we were a new nonprofit building technology to fight child sexual abuse.</s>
 <s id="10">But we were only two years old, and we had only worked on child sex trafficking.</s>
 <s id="11">And I had to tell them we had nothing.</s>
 <s id="12">We had nothing that could help them stop this abuse.</s>
 <s id="13">It took those agents another year to ultimately find that child.</s>
 <s id="14">And by the time she was rescued, hundreds of images and videos documenting her rape had gone viral, from the dark web to peer-to-peer networks, private chat rooms and to the websites you and I use every single day.</s>
 <s id="15">And today, as she struggles to recover, she lives with the fact that thousands around the world continue to watch her abuse.</s>
 <s id="16">I have come to learn in the last five years that this case is far from unique.</s>
 <s id="17">How did we get here as a society?</s>
 <s id="18">In the late 1980s, child pornography -- or what it actually is, child sexual abuse material -- was nearly eliminated.</s>
 <s id="19">New laws and increased prosecutions made it simply too risky to trade it through the mail.</s>
 <s id="20">And then came the internet, and the market exploded.</s>
 <s id="21">The amount of content in circulation today is massive and growing.</s>
 <s id="22">This is a truly global problem, but if we just look at the US: in the US alone last year, more than 45 million images and videos of child sexual abuse material were reported to the National Center for Missing and Exploited Children, and that is nearly double the amount the year prior.</s>
 <s id="23">And the details behind these numbers are hard to contemplate, with more than 60 percent of the images featuring children younger than 12, and most of them including extreme acts of sexual violence.</s>
 <s id="24">Abusers are cheered on in chat rooms dedicated to the abuse of children, where they gain rank and notoriety with more abuse and more victims.</s>
 <s id="25">In this market, the currency has become the content itself.</s>
 <s id="26">It's clear that abusers have been quick to leverage new technologies, but our response as a society has not.</s>
 <s id="27">These abusers don't read user agreements of websites, and the content doesn't honor geographic boundaries.</s>
 <s id="28">And they win when we look at one piece of the puzzle at a time, which is exactly how our response today is designed.</s>
 <s id="29">Law enforcement works in one jurisdiction.</s>
 <s id="30">Companies look at just their platform.</s>
 <s id="31">And whatever data they learn along the way is rarely shared.</s>
 <s id="32">It is so clear that this disconnected approach is not working.</s>
 <s id="33">We have to redesign our response to this epidemic for the digital age.</s>
 <s id="34">And that's exactly what we're doing at Thorn.</s>
 <s id="35">We're building the technology to connect these dots, to arm everyone on the front lines -- law enforcement, NGOs and companies -- with the tools they need to ultimately eliminate child sexual abuse material from the internet.</s>
 <s id="36">Let's talk for a minute -- (Applause) Thank you.</s>
 <s id="37">(Applause) Let's talk for a minute about what those dots are.</s>
 <s id="38">As you can imagine, this content is horrific.</s>
 <s id="39">If you don't have to look at it, you don't want to look at it.</s>
 <s id="40">And so, most companies or law enforcement agencies that have this content can translate every file into a unique string of numbers.</s>
 <s id="41">This is called a "hash."</s>
 <s id="42">It's essentially a fingerprint for each file or each video.</s>
 <s id="43">And what this allows them to do is use the information in investigations or for a company to remove the content from their platform, without having to relook at every image and every video each time.</s>
 <s id="44">The problem today, though, is that there are hundreds of millions of these hashes sitting in siloed databases all around the world.</s>
 <s id="45">In a silo, it might work for the one agency that has control over it, but not connecting this data means we don't know how many are unique.</s>
 <s id="46">We don't know which ones represent children who have already been rescued or need to be identified still.</s>
 <s id="47">So our first, most basic premise is that all of this data must be connected.</s>
 <s id="48">There are two ways where this data, combined with software on a global scale, can have transformative impact in this space.</s>
 <s id="49">The first is with law enforcement: helping them identify new victims faster, stopping abuse and stopping those producing this content.</s>
 <s id="50">The second is with companies: using it as clues to identify the hundreds of millions of files in circulation today, pulling it down and then stopping the upload of new material before it ever goes viral.</s>
 <s id="51">Four years ago, when that case ended, our team sat there, and we just felt this, um ...</s>
 <s id="52">... deep sense of failure, is the way I can put it, because we watched that whole year while they looked for her.</s>
 <s id="53">And we saw every place in the investigation where, if the technology would have existed, they would have found her faster.</s>
 <s id="54">And so we walked away from that and we went and we did the only thing we knew how to do: we began to build software.</s>
 <s id="55">So we've started with law enforcement.</s>
 <s id="56">Our dream was an alarm bell on the desks of officers all around the world so that if anyone dare post a new victim online, someone would start looking for them immediately.</s>
 <s id="57">I obviously can't talk about the details of that software, but today it's at work in 38 countries, having reduced the time it takes to get to a child by more than 65 percent.</s>
 <s id="58">(Applause) And now we're embarking on that second horizon: building the software to help companies identify and remove this content.</s>
 <s id="59">Let's talk for a minute about these companies.</s>
 <s id="60">So, I told you -- 45 million images and videos in the US alone last year.</s>
 <s id="61">Those come from just 12 companies.</s>
 <s id="62">Twelve companies, 45 million files of child sexual abuse material.</s>
 <s id="63">These come from those companies that have the money to build the infrastructure that it takes to pull this content down.</s>
 <s id="64">But there are hundreds of other companies, small- to medium-size companies around the world, that need to do this work, but they either: 1) can't imagine that their platform would be used for abuse, or 2) don't have the money to spend on something that is not driving revenue.</s>
 <s id="65">So we went ahead and built it for them, and this system now gets smarter with the more companies that participate.</s>
 <s id="66">Let me give you an example.</s>
 <s id="67">Our first partner, Imgur -- if you haven't heard of this company, it's one of the most visited websites in the US -- millions of pieces of user-generated content uploaded every single day, in a mission to make the internet a more fun place.</s>
 <s id="68">They partnered with us first.</s>
 <s id="69">Within 20 minutes of going live on our system, someone tried to upload a known piece of abuse material.</s>
 <s id="70">They were able to stop it, they pull it down, they report it to the National Center for Missing and Exploited Children.</s>
 <s id="71">But they went a step further, and they went and inspected the account of the person who had uploaded it.</s>
 <s id="72">Hundreds more pieces of child sexual abuse material that we had never seen.</s>
 <s id="73">And this is where we start to see exponential impact.</s>
 <s id="74">We pull that material down, it gets reported to the National Center for Missing and Exploited Children and then those hashes go back into the system and benefit every other company on it.</s>
 <s id="75">And when the millions of hashes we have lead to millions more and, in real time, companies around the world are identifying and pulling this content down, we will have dramatically increased the speed at which we are removing child sexual abuse material from the internet around the world.</s>
 <s id="76">(Applause) But this is why it can't just be about software and data, it has to be about scale.</s>
 <s id="77">We have to activate thousands of officers, hundreds of companies around the world if technology will allow us to outrun the perpetrators and dismantle the communities that are normalizing child sexual abuse around the world today.</s>
 <s id="78">And the time to do this is now.</s>
 <s id="79">We can no longer say we don't know the impact this is having on our children.</s>
 <s id="80">The first generation of children whose abuse has gone viral are now young adults.</s>
 <s id="81">The Canadian Centre for Child Protection just did a recent study of these young adults to understand the unique trauma they try to recover from, knowing that their abuse lives on.</s>
 <s id="82">Eighty percent of these young adults have thought about suicide.</s>
 <s id="83">More than 60 percent have attempted suicide.</s>
 <s id="84">And most of them live with the fear every single day that as they walk down the street or they interview for a job or they go to school or they meet someone online, that that person has seen their abuse.</s>
 <s id="85">And the reality came true for more than 30 percent of them.</s>
 <s id="86">They had been recognized from their abuse material online.</s>
 <s id="87">This is not going to be easy, but it is not impossible.</s>
 <s id="88">Now it's going to take the will, the will of our society to look at something that is really hard to look at, to take something out of the darkness so these kids have a voice; the will of companies to take action and make sure that their platforms are not complicit in the abuse of a child; the will of governments to invest with their law enforcement for the tools they need to investigate a digital first crime, even when the victims cannot speak for themselves.</s>
 <s id="89">This audacious commitment is part of that will.</s>
 <s id="90">It's a declaration of war against one of humanity's darkest evils.</s>
 <s id="91">But what I hang on to is that it's actually an investment in a future where every child can simply be a kid.</s>
 <s id="92">Thank you.</s>
 <s id="93">(Applause)</s>
</text>
